import os
import sys
import torch
from torch.utils.data import Dataset
import numpy as np
import nibabel as nib
import pandas as pd

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from config import IMAGE_DIR, REPORTS, LABELS


class CTMultimodalDataset(Dataset):
    def __init__(self, split="train", tokenizer=None, max_length=512, transform=None):
        super().__init__()
        self.split = split
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.transform = transform

        # è¯»å–æŠ¥å‘Šå’Œæ ‡ç­¾ CSV
        self.reports_df = pd.read_csv(REPORTS[split])
        self.labels_df = pd.read_csv(LABELS[split])

        # åˆ›å»ºæ–°åˆ—ï¼šæ‹¼æ¥æŠ¥å‘Šå­—æ®µ Findings_EN + Impressions_EN
        self.reports_df["report"] = (
            self.reports_df["Findings_EN"].fillna("") + " " +
            self.reports_df["Impressions_EN"].fillna("")
        )

        # æ„é€ æ ·æœ¬åˆ—è¡¨
        self.samples = self._merge_metadata()

    def _merge_metadata(self):
        report_dict = dict(zip(self.reports_df["VolumeName"], self.reports_df["report"]))
        label_dict = self.labels_df.set_index("VolumeName").to_dict(orient="index")

        common_ids = list(set(report_dict.keys()) & set(label_dict.keys()))
        print(f"âœ… åŒ¹é…æ ·æœ¬æ•°: {len(common_ids)} / æŠ¥å‘Šæ€»æ•°: {len(report_dict)} / æ ‡ç­¾æ€»æ•°: {len(label_dict)}")

        samples = []
        for vid in common_ids:
            samples.append({
                "id": vid,
                "report": report_dict[vid],
                "labels": np.array(list(label_dict[vid].values()), dtype=np.float32)
            })
        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        sample_id = sample["id"]
        patient_folder = "_".join(sample_id.split("_")[:2])  # e.g., train_1

        # å›¾åƒè·¯å¾„
        image_path = os.path.join(IMAGE_DIR, patient_folder, f"{sample_id}.nii.gz")
        image_data = nib.load(image_path).get_fdata().astype(np.float32)
        image_tensor = torch.from_numpy(image_data).unsqueeze(0)  # [1, D, H, W]

        if self.transform:
            image_tensor = self.transform(image_tensor)

        # æ–‡æœ¬ç¼–ç 
        if self.tokenizer:
            text_input = self.tokenizer(
                sample["report"],
                max_length=self.max_length,
                truncation=True,
                padding="max_length",
                return_tensors="pt"
            )
            text_input = {k: v.squeeze(0) for k, v in text_input.items()}
        else:
            text_input = sample["report"]

        return {
            "id": sample_id,
            "image": image_tensor,
            "text": text_input,
            "label": torch.tensor(sample["labels"])
        }


# âœ… æµ‹è¯•å…¥å£
if __name__ == "__main__":
    from transformers import AutoTokenizer
    from torch.utils.data import DataLoader
    import matplotlib.pyplot as plt

    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    dataset = CTMultimodalDataset(split="train", tokenizer=tokenizer)

    print(f"ğŸ“¦ æ ·æœ¬æ•°: {len(dataset)}")
    loader = DataLoader(dataset, batch_size=2, shuffle=True)

    batch = next(iter(loader))
    print("âœ… ID:", batch["id"])
    print("ğŸ–¼ï¸ å›¾åƒ shape:", batch["image"].shape)
    print("ğŸ“ æ–‡æœ¬ input_ids shape:", batch["text"]["input_ids"].shape)
    print("ğŸ·ï¸ æ ‡ç­¾ shape:", batch["label"].shape)

    # å¯è§†åŒ–ç¬¬ä¸€å¼ å›¾åƒçš„ä¸­é—´åˆ‡ç‰‡
    volume = batch["image"][0, 0].numpy()
    z = volume.shape[2] // 2
    plt.imshow(volume[:, :, z], cmap="gray")
    plt.title("ä¸­é—´åˆ‡ç‰‡")
    plt.axis("off")
    plt.show()