import os
import sys
import torch
from torch.utils.data import Dataset
import numpy as np
import nibabel as nib
import pandas as pd

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from config import IMAGE_DIR, REPORTS, LABELS


class CTMultimodalDataset(Dataset):
    def __init__(self, split="train", tokenizer=None, max_length=512, transform=None):
        super().__init__()
        self.split = split
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.transform = transform

        # è¯»å– CSV æ•°æ®
        self.reports_df = pd.read_csv(REPORTS[split])
        self.labels_df = pd.read_csv(LABELS[split])

        # æ„å»ºæŠ¥å‘Šå­—æ®µ
        self.reports_df["report"] = (
            self.reports_df["Findings_EN"].fillna("") + " " +
            self.reports_df["Impressions_EN"].fillna("")
        )

        self.samples = self._merge_metadata()

    def _merge_metadata(self):
        report_dict = dict(zip(self.reports_df["VolumeName"], self.reports_df["report"]))
        label_dict = self.labels_df.set_index("VolumeName").to_dict(orient="index")

        common_ids = list(set(report_dict.keys()) & set(label_dict.keys()))
        print(f"âœ… åŒ¹é…æ ·æœ¬æ•°: {len(common_ids)} / æŠ¥å‘Šæ€»æ•°: {len(report_dict)} / æ ‡ç­¾æ€»æ•°: {len(label_dict)}")

        samples = []
        for vid in common_ids:
            samples.append({
                "id": vid,  # e.g. train_1_a_1.nii.gz
                "report": report_dict[vid],
                "labels": np.array(list(label_dict[vid].values()), dtype=np.float32)
            })
        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        sample_id = sample["id"]  # e.g. train_1_a_1.nii.gz

        # æ–‡ä»¶åå»åç¼€å–è·¯å¾„ç»“æ„
        id_core = sample_id.replace(".nii.gz", "")  # train_1_a_1 â†’ ['train', '1', 'a', '1']
        parts = id_core.split("_")  # ['train', '1', 'a', '1']
        level1 = f"{parts[0]}_{parts[1]}"         # train_1
        level2 = f"{parts[0]}_{parts[1]}_{parts[2]}"  # train_1_a

        # æ„å»ºå®Œæ•´è·¯å¾„
        image_path = os.path.join(IMAGE_DIR, level1, level2, sample_id)

        if not os.path.exists(image_path):
            raise FileNotFoundError(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")

        image_data = nib.load(image_path).get_fdata().astype(np.float32)
        image_tensor = torch.from_numpy(image_data).unsqueeze(0)  # [1, D, H, W]

        if self.transform:
            image_tensor = self.transform(image_tensor)

        # æ–‡æœ¬ç¼–ç 
        if self.tokenizer:
            text_input = self.tokenizer(
                sample["report"],
                max_length=self.max_length,
                truncation=True,
                padding="max_length",
                return_tensors="pt"
            )
            text_input = {k: v.squeeze(0) for k, v in text_input.items()}
        else:
            text_input = sample["report"]

        return {
            "id": sample_id,
            "image": image_tensor,
            "text": text_input,
            "label": torch.tensor(sample["labels"])
        }


# âœ… æµ‹è¯•å…¥å£
if __name__ == "__main__":
    from transformers import AutoTokenizer
    from torch.utils.data import DataLoader
    import matplotlib.pyplot as plt

    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    dataset = CTMultimodalDataset(split="train", tokenizer=tokenizer)

    print(f"ğŸ“¦ æ ·æœ¬æ•°: {len(dataset)}")
    
    '''
    loader = DataLoader(dataset, batch_size=2, shuffle=True)

    batch = next(iter(loader))
    print("âœ… ID:", batch["id"])
    print("ğŸ–¼ï¸ å›¾åƒ shape:", batch["image"].shape)
    print("ğŸ“ æ–‡æœ¬ input_ids shape:", batch["text"]["input_ids"].shape)
    print("ğŸ·ï¸ æ ‡ç­¾ shape:", batch["label"].shape)
    # å¯è§†åŒ–ç¬¬ä¸€å¼ å›¾åƒçš„ä¸­é—´åˆ‡ç‰‡
    volume = batch["image"][0, 0].numpy()
    z = volume.shape[2] // 2
    plt.imshow(volume[:, :, z], cmap="gray")
    plt.title("ä¸­é—´åˆ‡ç‰‡")
    plt.axis("off")
    plt.show()
    '''
    
    # æŸ¥æ‰¾ç›®æ ‡æ ·æœ¬ç´¢å¼•
    target_id = "train_1_a_1.nii.gz"
    idx = next((i for i, s in enumerate(dataset.samples) if s["id"] == target_id), None)

    if idx is None:
        print(f"âŒ æœªæ‰¾åˆ°æ ·æœ¬ {target_id}")
    else:
        sample = dataset[idx]
        print("âœ… æ ·æœ¬ ID:", sample["id"])
        print("ğŸ“ æŠ¥å‘Šå‰200å­—:\n", sample["text"]["input_ids"][:10], "...")  # or sample["report"]

        # å¯è§†åŒ–å›¾åƒä¸­é—´å±‚
        volume = sample["image"][0].numpy()
        z = volume.shape[2] // 2
        plt.imshow(volume[:, :, z], cmap="gray")
        plt.title(f"{target_id} - ä¸­é—´åˆ‡ç‰‡")
        plt.axis("off")
        plt.show()