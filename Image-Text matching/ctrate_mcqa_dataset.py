import os
import json
import torch
from torch.utils.data import Dataset, DataLoader
import nibabel as nib
from transformers import AutoTokenizer

def center_crop_depth(image, target_depth=128):
    _, D, H, W = image.shape
    if D < target_depth:
        # Padding if too small
        pad = (target_depth - D) // 2
        pad_left = torch.zeros((1, pad, H, W))
        pad_right = torch.zeros((1, target_depth - D - pad, H, W))
        image = torch.cat([pad_left, image, pad_right], dim=1)
    elif D > target_depth:
        # Crop center
        start = (D - target_depth) // 2
        image = image[:, start:start+target_depth, :, :]
    return image

import torch.nn.functional as F

def resize_hw(image, target_hw):
    """
    image: [1, D, H, W]
    returns: resized image [1, D, target_H, target_W]
    """
    _, D, H, W = image.shape
    image = image.permute(1, 0, 2, 3)  # [D, 1, H, W]
    image = F.interpolate(image, size=target_hw, mode='bilinear', align_corners=False)
    image = image.permute(1, 0, 2, 3)  # [1, D, H, W]
    return image


class CTReportMCQADataset(Dataset):
    def __init__(self, jsonl_path, tokenizer_name="bert-base-uncased", max_length=512, image_transform=None):
        self.samples = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                self.samples.append(json.loads(line))
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.max_length = max_length
        self.image_transform = image_transform

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        entry = self.samples[idx]
        image_path = entry["image_path"]
        metadata_prompt = entry["metadata_prompt"]
        options = entry["options"]
    
        # âœ… å›¾åƒåŠ è½½
        img_nii = nib.load(image_path)
        image = img_nii.get_fdata()
        image = torch.tensor(image).unsqueeze(0).float()  # [1, D, H, W]
    
        # âœ… ä¿®æ”¹å°ºå¯¸ï¼ˆæ›´å°ã€æé€Ÿï¼‰
        #image = center_crop_depth(image, target_depth=64)            # ä¸­å¿ƒè£å‰ªæ·±åº¦
        #image = resize_hw(image, target_hw=(128, 128))               # resize é«˜å®½
        image = center_crop_depth(image, target_depth=32)
        image = resize_hw(image, target_hw=(96, 96))  # æ›´å°ç©ºé—´åˆ†è¾¨ç‡
        
        if self.image_transform:
            image = self.image_transform(image)
        
        # æ–‡æœ¬å¤„ç†
        input_ids = []
        attention_masks = []
        label_index = -1

        for i, opt in enumerate(options):
            #ä¸è¦ prompt è¯•è¯•
            #full_text = metadata_prompt + " " + opt["text"]
            full_text = opt["text"]
            encoded = self.tokenizer(
                full_text,
                padding="max_length",
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt"
            )
            input_ids.append(encoded["input_ids"].squeeze(0))
            attention_masks.append(encoded["attention_mask"].squeeze(0))
            if opt["label"] == 1:
                label_index = i

        input_ids = torch.stack(input_ids)             # [4, N]
        attention_masks = torch.stack(attention_masks) # [4, N]

        return {
            "image": image,
            "input_ids": input_ids,
            "attention_mask": attention_masks,
            "label": torch.tensor(label_index)
        }

# -------------------------------------------
# âœ… ç¤ºä¾‹ä¸»å‡½æ•°ï¼šç›´æ¥è¿è¡Œæ–‡ä»¶æµ‹è¯•æ•°æ®åŠ è½½
# -------------------------------------------
if __name__ == "__main__":
    jsonl_path = "ctrate_mcqa_full_report_train.jsonl"  # è¯·ç¡®è®¤è·¯å¾„å­˜åœ¨

    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®é›†: {jsonl_path}")
    dataset = CTReportMCQADataset(
        jsonl_path=jsonl_path,
        tokenizer_name="bert-base-uncased",
        max_length=256
    )

    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

    for batch in dataloader:
        print("âœ… æ‰¹æ•°æ®åŠ è½½æˆåŠŸ!")
        print("å›¾åƒå¼ é‡ image.shape:", batch["image"].shape)               # [B, 1, D, H, W]
        print("æ–‡æœ¬ input_ids.shape:", batch["input_ids"].shape)          # [B, 4, N]
        print("æ ‡ç­¾ label:", batch["label"])                              # [B]
        
        # ğŸ” å¯è§†åŒ–æ¯ä¸ªé€‰é¡¹çš„ token åŸæ–‡
        print("\nğŸ“„ æ¯ä¸ªé€‰é¡¹åŸå§‹æ–‡æœ¬ï¼ˆå»tokenåŒ–ï¼‰ï¼š")
        tokenizer = dataset.tokenizer
        for i in range(4):
            decoded = tokenizer.decode(batch["input_ids"][0][i], skip_special_tokens=True)
            print(f"Option {i}: {decoded}")

        print("\nğŸ¯ æ­£ç¡®é€‰é¡¹ç´¢å¼•:", batch["label"].item())

        break  # åªæŸ¥çœ‹ä¸€æ‰¹æ ·æœ¬
