# CT-CLIP 和 CLIP

### **CT-CLIP 和 CLIP 做图文匹配有什么区别？**

**CT-CLIP vs CLIP：核心区别在于 专业性 和 结构适配性**

| 特征     | CT-CLIP                                    | CLIP（OpenAI）                                    |
| -------- | ------------------------------------------ | ------------------------------------------------- |
| 训练数据 | 专门在 CT-RATE 上训练，医学CT图像 + 报告   | 普通自然图像 + 网络文本（如 captions）            |
| 图像输入 | **3D CT 体积**（支持 volumetric 特征）     | **2D 图像**，不支持 3D 医学图像                   |
| 文本风格 | 医学术语、放射报告                         | 网络语言、短文本、图像描述                        |
| 模型结构 | 3D Vision Encoder + Transformer 文本编码器 | 2D Vision Encoder（如 ResNet/ViT） + Text Encoder |
| 应用适配 | 专为医学报告匹配、检索优化                 | 广泛适用于自然图像/描述配对                       |

**CT-CLIP 是专为你这类数据设计的，效果和泛化能力都优于原始 CLIP。**

如果你用的是 **3D CT 图像 + 放射科报告**，CLIP（OpenAI版）很难适配，除非你大改输入。

------

### CT-CLIP 已经做过图文匹配，我还可以做吗？

——答案是**绝对可以！而且你有几个切入点**：

| 创新点           | 举例                                                 | 是否跟 CT-CLIP 不一样 |
| ---------------- | ---------------------------------------------------- | --------------------- |
| 📥 输入文本不同   | 不用 full report，而是结构化关键词 / impression-only | ✅                     |
| 🎯 任务目标不同   | 不止匹配，还要“检索”、“选择正确解释”                 | ✅                     |
| 🔄 模型结构变化   | 用 LLaVA、BLIP-2 或加入 metadata                     | ✅                     |
| 🔍 Prompt引导匹配 | “这张图是否包含肺结节？”→ yes/no                     | ✅                     |
| 📶 多任务训练     | 图文匹配 + 多标签分类同时做                          | ✅                     |
| 📦 扩展任务       | Text→Image 反向检索，或局部匹配                      | ✅                     |
| 📉 自监督优化     | 用 hard negative mining 或 N-pair contrastive loss   | ✅                     |

> 设计一个多模态选择题任务

- 给一张 CT 图像
- 提供 1 份真实报告 + 3 份伪报告（负样本）
- 让模型选出哪一份是匹配的（top-1 accuracy）

这不是 CT-CLIP 原始 paper 中做的，但它基于同样的嵌入学习原理，可以扩展 VQA、检索等任务

| 如果你是...                                | 建议                                          |
| ------------------------------------------ | --------------------------------------------- |
| 想复现原始 CT-CLIP 实验                    | 可以直接用官方模型 / 用你自己的报告格式做对比 |
| 想在 CT-RATE 上做出**自己的图文匹配方向**  | 加结构化字段、换报告结构、换任务目标、换 loss |
| 想探索图文对比学习 + 下游任务（分类/生成） | 在 CT-CLIP 基础上扩展非常合适                 |

------

# 多模态选择题匹配任务

我们现在建立一个多模态选择题匹配任务（Multimodal Report Discrimination）

> 给一张 CT 图像，和 4 个选项（1 个真实报告 + 3 个伪报告），让模型选出正确的那一个。

这个形式：

- 非常贴近 **人类问诊/辅助诊断场景**
- 非常适合用 **对比学习、检索学习、选择型QA、CLIP风格模型**
- 是 CT-CLIP paper 没有明确做过的（创新点！）

## 如何构建这个任务？

#### 1. 准备数据（生成 1 正 + N 负 配对）

你可以用之前处理过的 `df_final`（已经保留 image + full_report），然后这样构造每条样本：

```json
{
  "image_path": "D:/.../train_3_a_1.nii.gz",
  "options": [
    {"text": "真实报告", "label": 1},
    {"text": "随机假报告1", "label": 0},
    {"text": "随机假报告2", "label": 0},
    {"text": "随机假报告3", "label": 0}
  ]
}
```

> **Tip**：你可以控制负报告与正报告相似/不相似，提高任务难度。

------

#### 2. 引入**结构化关键词 / Impression-only** 作为选项文本变种

你可以设计两个版本的任务（创新点一）：

| 模式                | 选项内容                                           | 特点                             |
| ------------------- | -------------------------------------------------- | -------------------------------- |
| **Full Report**     | Findings + Impressions 拼接                        | 内容丰富                         |
| **Impression Only** | 只用 `Impressions_EN`                              | 精简、重点突出                   |
| **结构关键词形式**  | 自动抽取的关键词组成（如：lung, effusion, nodule） | 类似 QA Prompt，可加 prompt 形式 |

💡 可以混合训练或评估不同形式下模型的鲁棒性。

------

#### 3. 加入 metadata / prompt / 模型创新

你可以把 metadata（如年龄、性别、设备型号等）加入“上下文提示”或模型结构中：

**方式 1：作为 Prompt 拼进选项**

```text
"Age: 67, Sex: M. Impression: Calcific plaques in coronary arteries."
```

**方式 2：作为多模态输入（通过 encoder 输入模型）**

你可以这样送入模型：

```python
image → VisionEncoder
text_option → TextEncoder
metadata → MLP or token embedding
→ score = fusion(image + text + metadata)
```

------

## 模型方面：你可以怎么训练它？

1. **双塔模型（双 encoder）**：
   - 图像 → Vision Encoder
   - 文本选项 → Text Encoder
   - 计算 cosine similarity → 选择最高的选项
2. **BLIP-2 / LLaVA风格的多模态 QA 模型**：
   - image + prompt → decoder → 输出 index（0~3）
   - 更适合多轮对话或更复杂场景
3. **Contrastive Learning 风格**：
   - 训练模型拉近 image 与正确 report 的 embedding，远离负样本

| 步骤                  | 动作                                     |
| --------------------- | ---------------------------------------- |
| ✅ 1. 构建选项数据集   | 为每张图像构建 1 正 + 3 负报告项         |
| ✅ 2. 尝试不同选项风格 | full report / impression only / keyword  |
| ✅ 3. 加 metadata      | 作为上下文提示 or 模型输入               |
| ✅ 4. 模型训练         | 轻量双塔 or BLIP-style QA 模型           |
| ✅ 5. 指标评估         | Top-1 Accuracy, Recall@k, Hard Negatives |

------

# Image encoder

> 总体流程：JSONL 数据 ➜ 图像+文本编码 ➜ 模型 forward ➜ 多选预测 ➜ 计算 loss

### ViT 是什么？

**ViT（Vision Transformer）** 是由 Google 提出的视觉版 Transformer。

- 把图像切成 patch（如 16×16），每个 patch 当作一个“token”
- 输入到 transformer encoder 里做自注意力
- 不同于 CNN，ViT 更擅长建模**全局上下文**，尤其适合对图像整体语义理解的任务（比如“这张图描述了什么”）

| 项目              | CLIPVisionModel                                              | ViT                                                     |
| ----------------- | ------------------------------------------------------------ | ------------------------------------------------------- |
| 来源              | OpenAI 的 CLIP 模型中自带的视觉编码器                        | 原始 Vision Transformer（Google 提出）                  |
| 本质上是          | 一个预训练好的 ViT（比如 ViT-B/16）                          | 原始 ViT 架构，也有各种版本                             |
| 是否预训练        | ✅ 已在大规模图文对上训练（CLIP）                             | 可能没有预训练，也可能有 ImageNet                       |
| 特点              | 适合做图文匹配/对比学习                                      | 通用图像理解模型，任务需要 finetune                     |
| 在 HuggingFace 中 | CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch16") | ViTModel.from_pretrained("google/vit-base-patch16-224") |

**这里选择 CLIPVisionModel。**

### 模型搭建流程

> 图像预处理 ➜ 图像切片（或整体） ➜ 编码成一个向量 ➜ 与文本嵌入匹配

详细流程：

1. **加载图像**：`.nii.gz`（3D CT）或 `.png/.jpg`（2D切片）
2. **预处理**：归一化、resize、选取 slice（中间、最大病灶、平均投影等）
3. **编码**：输入 vision encoder（CLIP、ViT、CNN），输出 `[B, D]` 向量
4. **与文本融合**：多模态匹配（dot-product、cross-attn等）

现在做的是：

> 多模态选择题匹配任务：图像 + 4个文本候选项 → 选出最匹配的文本

| 目标                         | 建议输入          | 原因                               |
| ---------------------------- | ----------------- | ---------------------------------- |
| 快速验证/轻量训练            | **中间切片 (2D)** | 更容易用 ViT/CLIP，训练快，GPU友好 |
| 更高精度 / 空间完整性        | **3D volume**     | 捕捉更完整解剖结构，但耗显存       |
| 可对比已有方法（如 CT-CLIP） | 3D 更适配         | 他们用的是 3D CLIP-style 模型      |

| 情况                          | 建议方式                                  |
| ----------------------------- | ----------------------------------------- |
| 你要先试通流程 / 精调模型结构 | 先用中间切片 + CLIPVisionModel            |
| 你要训练医学专用模型          | 可用 MONAI 或 SwinUNETR 加 3D volume 输入 |
| 你将来要扩展做报告生成 / 分类 | 用 3D volume 更全面                       |

**建议（实际操作）：**

第一阶段：快速跑通中间切片 + CLIPVisionModel + 多模态选择题任务

第二阶段：用 3D volume + Simple3DCNN 搭一个正式训练 pipeline

## 3D volume + Simple3DCNN

1. ✅ 使用你已有的 `.nii.gz` 图像 volume（不用提切片）
2. ✅ 使用 `Simple3DCNN` 做 image encoder
3. ✅ 使用 `BertModel` 做 text encoder
4. ✅ 构建 `MultimodalMCQAModel` 模型（图文对 → 匹配分数）
5. ✅ 构建训练循环：前向、loss、优化器、打印训练结果
