import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
from ctrate_mcqa_dataset import CTReportMCQADataset
from multimodal_mcqa_model import MultimodalMCQAModel

from torchvision.models.video import r3d_18

# âœ… ä½¿ç”¨ ResNet3D ä½œä¸º vision encoder
class ResNet3DEncoder(nn.Module):
    def __init__(self, out_dim=768, pretrained=False, debug=False):
        super().__init__()
        self.debug = debug
        self.backbone = r3d_18(pretrained=pretrained)
        self.backbone.fc = nn.Identity()  # [B, 512]
        self.projector = nn.Sequential(
            nn.Linear(512, out_dim),
            nn.LayerNorm(out_dim)
        )

    def forward(self, x):  # x: [B, 1, D, H, W]
        if x.shape[1] == 1:
            x = x.repeat(1, 3, 1, 1, 1)  # [B, 3, D, H, W]

        if self.debug:
            print(f"[Input to ResNet3D] {x.shape}")

        x = self.backbone(x)  # [B, 512]
        if self.debug:
            print(f"[Backbone out] {x.shape}")

        x = self.projector(x)  # [B, out_dim]
        if self.debug:
            print(f"[Final encoded] {x.shape}")
        return x

# âœ… è®­ç»ƒå‚æ•°
BATCH_SIZE = 2
EPOCHS = 3
MAX_LEN = 256
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… åŠ è½½æ•°æ®é›†
def get_dataloaders():
    train_dataset = CTReportMCQADataset("ctrate_mcqa_full_report_train.jsonl", max_length=MAX_LEN)
    val_dataset = CTReportMCQADataset("ctrate_mcqa_full_report_val.jsonl", max_length=MAX_LEN)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    return train_loader, val_loader

# âœ… éªŒè¯é€»è¾‘
def evaluate(model, dataloader):
    model.eval()
    total_loss = 0
    total_correct = 0
    total_samples = 0

    with torch.no_grad():
        for batch in dataloader:
            image = batch["image"].to(DEVICE)
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)
            labels = batch["label"].to(DEVICE)

            logits = model(image, input_ids, attention_mask)
            loss = nn.functional.cross_entropy(logits, labels)

            total_loss += loss.item()
            total_correct += (logits.argmax(dim=1) == labels).sum().item()
            total_samples += labels.size(0)

    avg_loss = total_loss / len(dataloader)
    accuracy = total_correct / total_samples
    return avg_loss, accuracy

# âœ… å¼€å§‹è®­ç»ƒ
def train():
    train_loader, val_loader = get_dataloaders()

    vision_encoder = ResNet3DEncoder(out_dim=768, pretrained=False, debug=False)

    model = MultimodalMCQAModel(vision_encoder, text_encoder_name="bert-base-uncased")
    model = model.to(DEVICE)

    optimizer = optim.AdamW(model.parameters(), lr=2e-5)

    # âœ… æ¨¡å‹ä¿å­˜ & early stopping è®¾ç½®
    best_val_acc = 0.0
    patience = 2
    patience_counter = 0

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        correct = 0
        total = 0

        for batch in tqdm(train_loader, desc=f"[Train] Epoch {epoch+1}"):
            image = batch["image"].to(DEVICE)
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)
            labels = batch["label"].to(DEVICE)

            logits = model(image, input_ids, attention_mask)
            loss = nn.functional.cross_entropy(logits, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            correct += (logits.argmax(dim=1) == labels).sum().item()
            total += labels.size(0)

        train_acc = correct / total
        train_loss = total_loss / len(train_loader)

        # âœ… éªŒè¯
        val_loss, val_acc = evaluate(model, val_loader)

        print(f"âœ… Epoch {epoch+1} Summary:")
        print(f"   ğŸ”¹ Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
        print(f"   ğŸ”¹ Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}")

        # âœ… æ¨¡å‹ä¿å­˜ï¼ˆåªä¿å­˜æœ€ä¼˜ï¼‰
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            torch.save(model.state_dict(), "best_mcqa_model.pt")
            print(f"ğŸ“¦ æ¨¡å‹å·²ä¿å­˜ï¼ï¼ˆval acc æå‡è‡³ {val_acc:.4f}ï¼‰")
        else:
            patience_counter += 1
            print(f"â¸ï¸ æ²¡æœ‰æå‡ï¼Œearly stoppingè®¡æ•°: {patience_counter}/{patience}")

        # âœ… Early stopping è§¦å‘
        if patience_counter >= patience:
            print("ğŸ›‘ è§¦å‘ early stoppingï¼Œè®­ç»ƒç»“æŸã€‚")
            break

if __name__ == "__main__":
    train()
